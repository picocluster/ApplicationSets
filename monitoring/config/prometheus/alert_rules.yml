# Prometheus Alert Rules for PicoCluster
#
# Usage:
#   1. Copy to /etc/prometheus/alert_rules.yml on monitoring node
#   2. Add to prometheus.yml:
#        rule_files:
#          - /etc/prometheus/alert_rules.yml
#   3. Restart Prometheus: sudo systemctl restart prometheus
#   4. Verify alerts: http://localhost:9090/alerts
#
# Alert severity levels:
#   - critical: Immediate action required
#   - warning:  Should be addressed soon
#   - info:     Informational only

groups:
  - name: picocluster_alerts
    interval: 30s
    rules:

      # ==================== Node Health ====================

      - alert: NodeDown
        expr: up{job="node"} == 0
        for: 5m
        labels:
          severity: critical
          service: infrastructure
        annotations:
          summary: "Node {{ $labels.node }} is down"
          description: "Node {{ $labels.node }} ({{ $labels.instance }}) has been unreachable for 5 minutes."
          runbook_url: "https://github.com/picocluster/ApplicationSets/blob/master/docs/troubleshooting.md#node-down"

      - alert: NodeUnreachableFor30Minutes
        expr: up{job="node"} == 0
        for: 30m
        labels:
          severity: critical
          service: infrastructure
        annotations:
          summary: "Node {{ $labels.node }} has been down for 30 minutes"
          description: "Node {{ $labels.node }} has been unreachable for 30 minutes. Check physical connection and power."

      - alert: MultipleNodesDown
        expr: count(up{job="node"} == 0) >= 2
        for: 5m
        labels:
          severity: critical
          service: infrastructure
        annotations:
          summary: "{{ $value }} nodes are down"
          description: "{{ $value }} cluster nodes are unreachable. Potential network issue."

      # ==================== CPU Usage ====================

      - alert: HighCPUUsage
        expr: 100 - (avg by (node) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
        for: 5m
        labels:
          severity: warning
          service: node
        annotations:
          summary: "High CPU usage on {{ $labels.node }}"
          description: "Node {{ $labels.node }} CPU usage is {{ $value | humanizePercentage }} (threshold: 85%)"

      - alert: CriticalCPUUsage
        expr: 100 - (avg by (node) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
        for: 2m
        labels:
          severity: critical
          service: node
        annotations:
          summary: "Critical CPU usage on {{ $labels.node }}"
          description: "Node {{ $labels.node }} CPU usage is {{ $value | humanizePercentage }} (threshold: 95%)"

      # ==================== Memory Usage ====================

      - alert: HighMemoryUsage
        expr: 100 * (1 - ((node_memory_MemAvailable_bytes or (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes)) / node_memory_MemTotal_bytes)) > 85
        for: 5m
        labels:
          severity: warning
          service: node
        annotations:
          summary: "High memory usage on {{ $labels.node }}"
          description: "Node {{ $labels.node }} memory usage is {{ $value | humanizePercentage }} (threshold: 85%)"

      - alert: CriticalMemoryUsage
        expr: 100 * (1 - ((node_memory_MemAvailable_bytes or (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes)) / node_memory_MemTotal_bytes)) > 95
        for: 2m
        labels:
          severity: critical
          service: node
        annotations:
          summary: "Critical memory usage on {{ $labels.node }}"
          description: "Node {{ $labels.node }} memory usage is {{ $value | humanizePercentage }} (threshold: 95%)"

      - alert: OutOfMemory
        expr: node_memory_MemAvailable_bytes < 104857600  # 100MB
        for: 2m
        labels:
          severity: critical
          service: node
        annotations:
          summary: "Out of memory on {{ $labels.node }}"
          description: "Node {{ $labels.node }} has less than 100MB available memory"

      # ==================== Disk Space ====================

      - alert: DiskSpaceWarning
        expr: 100 * (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) > 80
        for: 5m
        labels:
          severity: warning
          service: node
        annotations:
          summary: "Low disk space on {{ $labels.node }}"
          description: "Node {{ $labels.node }} disk usage is {{ $value | humanizePercentage }} (threshold: 80%)"

      - alert: DiskSpaceCritical
        expr: 100 * (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) > 90
        for: 2m
        labels:
          severity: critical
          service: node
        annotations:
          summary: "Critical disk space on {{ $labels.node }}"
          description: "Node {{ $labels.node }} disk usage is {{ $value | humanizePercentage }} (threshold: 90%)"

      - alert: DiskFull
        expr: 100 * (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) >= 98
        for: 1m
        labels:
          severity: critical
          service: node
        annotations:
          summary: "DISK FULL on {{ $labels.node }}"
          description: "Node {{ $labels.node }} is {{ $value | humanizePercentage }} full. Immediate action required!"

      - alert: InodeSpaceWarning
        expr: 100 * (1 - (node_filesystem_files_free{mountpoint="/"} / node_filesystem_files{mountpoint="/"})) > 80
        for: 5m
        labels:
          severity: warning
          service: node
        annotations:
          summary: "Low inode space on {{ $labels.node }}"
          description: "Node {{ $labels.node }} inode usage is {{ $value | humanizePercentage }} (threshold: 80%)"

      # ==================== Network ====================

      - alert: HighNetworkPacketLoss
        expr: (rate(node_network_receive_errs_total[5m]) + rate(node_network_transmit_errs_total[5m])) > 100
        for: 5m
        labels:
          severity: warning
          service: network
        annotations:
          summary: "High network packet loss on {{ $labels.node }}"
          description: "Node {{ $labels.node }} is experiencing high packet loss ({{ $value }} errors/sec)"

      - alert: NetworkInterfaceDown
        expr: node_network_up{device!="lo"} == 0
        for: 5m
        labels:
          severity: warning
          service: network
        annotations:
          summary: "Network interface down on {{ $labels.node }}"
          description: "Interface {{ $labels.device }} on node {{ $labels.node }} is down"

      # ==================== Prometheus ====================

      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 5m
        labels:
          severity: critical
          service: monitoring
        annotations:
          summary: "Prometheus is down"
          description: "Prometheus monitoring service is not responding"

      - alert: PrometheusHighScrapeErrorRate
        expr: (rate(prometheus_sd_consul_rpc_failures_total[5m]) or rate(prometheus_http_request_duration_seconds_bucket{le="+Inf"}[5m])) > 0.05
        for: 10m
        labels:
          severity: warning
          service: monitoring
        annotations:
          summary: "Prometheus high scrape error rate"
          description: "Prometheus is experiencing high error rates when scraping targets"

      - alert: PrometheusConfigReloadFailure
        expr: prometheus_config_last_reload_successful == 0
        for: 5m
        labels:
          severity: critical
          service: monitoring
        annotations:
          summary: "Prometheus config reload failed"
          description: "Prometheus failed to reload configuration. Check logs for details."

      - alert: PrometheusDatastorageSpaceRunningOut
        expr: (prometheus_tsdb_symbol_table_size_bytes + prometheus_tsdb_data_compacted_block_bytes + prometheus_tsdb_wal_segment_bytes) * 1.2 > 5368709120  # 5GB
        for: 5m
        labels:
          severity: warning
          service: monitoring
        annotations:
          summary: "Prometheus storage running low"
          description: "Prometheus will run out of disk space for TSDB soon. Consider increasing retention or cleanup"

      # ==================== Grafana ====================

      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 5m
        labels:
          severity: critical
          service: monitoring
        annotations:
          summary: "Grafana is down"
          description: "Grafana dashboard service is not responding on port 3000"

      # ==================== Docker ====================

      - alert: DockerDaemonDown
        expr: up{job="docker"} == 0
        for: 5m
        labels:
          severity: warning
          service: docker
        annotations:
          summary: "Docker daemon is not responding on {{ $labels.node }}"
          description: "Docker daemon on {{ $labels.node }} is not responding to metrics requests"

      - alert: HighContainerRestarts
        expr: (rate(container_last_seen[5m]) + rate(container_last_seen offset 5m[5m])) > 10
        for: 5m
        labels:
          severity: warning
          service: docker
        annotations:
          summary: "High container restart rate on {{ $labels.node }}"
          description: "Containers are restarting frequently on {{ $labels.node }}"

      # ==================== Kubernetes ====================

      - alert: KubeletDown
        expr: up{job="kubelet"} == 0
        for: 5m
        labels:
          severity: critical
          service: kubernetes
        annotations:
          summary: "Kubelet down on {{ $labels.node }}"
          description: "Kubelet on node {{ $labels.node }} is not responding"

      - alert: KubePodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0.1
        for: 5m
        labels:
          severity: warning
          service: kubernetes
        annotations:
          summary: "Pod {{ $labels.pod }} is crashing in {{ $labels.namespace }}"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting frequently"

      - alert: KubeNodeNotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
          service: kubernetes
        annotations:
          summary: "Kubernetes node not ready: {{ $labels.node }}"
          description: "Kubernetes node {{ $labels.node }} is not in Ready state"

      - alert: KubeMemoryPressure
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
        for: 5m
        labels:
          severity: warning
          service: kubernetes
        annotations:
          summary: "Memory pressure on Kubernetes node {{ $labels.node }}"
          description: "Kubernetes node {{ $labels.node }} is experiencing memory pressure"

      - alert: KubeDiskPressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        for: 5m
        labels:
          severity: warning
          service: kubernetes
        annotations:
          summary: "Disk pressure on Kubernetes node {{ $labels.node }}"
          description: "Kubernetes node {{ $labels.node }} is experiencing disk pressure"

      # ==================== System ====================

      - alert: SystemLoadHigh
        expr: node_load5 > (count(node_cpu_seconds_total{mode="system"}) by (node))
        for: 5m
        labels:
          severity: warning
          service: node
        annotations:
          summary: "High system load on {{ $labels.node }}"
          description: "5-minute load average on {{ $labels.node }} is {{ $value }} (CPUs: {{ count }})"

      - alert: TooManyOpenFiles
        expr: (node_filefd_allocated / node_filefd_maximum) > 0.9
        for: 5m
        labels:
          severity: warning
          service: node
        annotations:
          summary: "Too many open files on {{ $labels.node }}"
          description: "Node {{ $labels.node }} is using {{ $value | humanizePercentage }} of available file descriptors"

      # ==================== Service Availability ====================

      - alert: NodeExporterDown
        expr: up{job="node"} == 0
        for: 5m
        labels:
          severity: warning
          service: monitoring
        annotations:
          summary: "Node Exporter down on {{ $labels.node }}"
          description: "Node Exporter on {{ $labels.node }} is not responding"

      - alert: TargetDown
        expr: up{job!="prometheus",job!="grafana"} == 0
        for: 10m
        labels:
          severity: warning
          service: monitoring
        annotations:
          summary: "Monitoring target down: {{ $labels.job }}"
          description: "Target {{ $labels.instance }} for job {{ $labels.job }} has been down for 10 minutes"
